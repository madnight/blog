<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=10.0">
    <meta name="author" content="Fabian Beuke">
    <link href="/fav.png" as="image" onload="this.rel='shortcut icon'" sizes="45x48">
    <noscript>
        <link rel="shortcut icon" sizes="45x48" href="/fav.png">
    </noscript>
    <link rel="icon" href="/fav.png" onload="if(media!='all')media='all'">
    <noscript>
        <link rel="icon" href="/fav.png">
    </noscript>
    <title>beuke.org</title>
    <meta name="description" content="A personal blog about computer science and theoretical physics.">
    <link rel="alternate" type="application/rss+xml" title="beuke.org" href="/atom.xml" onload="if(media!='all')media='all'">
    <noscript>
        <link rel="alternate" type="application/rss+xml" title="beuke.org" href="/atom.xml">
    </noscript>
    <link rel="preload" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" as="style">
    <link rel="stylesheet" href="/css/simple.css/simple.min.css" as="style">
    <link rel="stylesheet" href="/css/main.css" onload="if(media!='all')media='all'">
    <noscript>
        <link rel="stylesheet" href="/css/main.css">
    </noscript>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" media="none" onload="if(media!='all')media='all'">
    <noscript>
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
    </noscript>
    <link rel="stylesheet" href="/css/light.css" media="(prefers-color-scheme: light)">
    <link rel="stylesheet" href="/css/dark.css" media="(prefers-color-scheme: dark)">
    <!--<script src="https://googlechromelabs.github.io/dark-mode-toggle/src/dark-mode-toggle-stylesheets-loader.js"></script>-->
    <script data-goatcounter="https://stats.beuke.org/count" async src="/js/goatcounter.js"></script>
    <script type="module" src="https://unpkg.com/dark-mode-toggle"></script>
    <script defer type="text/javascript">
        document.addEventListener("DOMContentLoaded", function (event)
        {
            //- fix for single quotes in pre blocks
            document.body.innerHTML = document.body.innerHTML.replaceAll('', '\'');
        });
    </script>
    <script defer src="/js/table-wrapper.js"></script>
    <meta name="generator" content="Hexo 6.3.0">
</head>


    <header>
    <nav>
        
        <a href="/"> Home</a>
        
        <a href="https://github.com/madnight"> Github</a>
        
        <a href="/archive/"> Archive</a>
        
        <a href="/about/"> About</a>
        
    </nav>
<div class="title-con">
    <div class="title-container">
        <h1>beuke.org</h1>
        <dark-mode-toggle id="dark-mode-toggle-1" appearance="toggle"></dark-mode-toggle>
    </div>
</div>
    <h2>A personal blog exploring computer science and theoretical physics</h2>
</header>


    <body>
    <main>
            <article>
    <section>
        <div class="post-title"> Efficiently Updatable Neural Network (NNUE) </div>
        <div class="post-subtitle"> Development of Neural Network Chess Engines </div>
        <div class="post-subsubtitle"> Posted on
        Oct 7 2025 ~
        33 min read</div>
        <div class="post-subtitle-tags">
        
        <a href="/tags/chess/">#chess</a>¬†
        
        <a href="/tags/artificial-intelligence/">#artificial intelligence</a>¬†
        
        <a href="/tags/neural-networks/">#neural networks</a>¬†
        
        </div>
        
        <hr class="solid">
    </section>
    <div class"content">
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><audio controls>
  <source src="/audio/nnue.mp3" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>
<h2 id="origins-and-development-of-nnue-in-chess-engines"><a class="markdownIt-Anchor" href="#origins-and-development-of-nnue-in-chess-engines"></a> Origins and Development of NNUE in Chess Engines</h2>
<p>The Efficiently Updatable Neural Network (NNUE) originated in the Japanese computer shogi community. It was invented in 2018 by Yu Nasu, who introduced this neural-network-based evaluation approach to shogi as a replacement for traditional handcrafted evaluation functions<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. The name ‚ÄúNNUE‚Äù is a Japanese wordplay on Nue (a mythical chimera), and it is sometimes stylized as ‚Äú∆éU–ò–ò‚Äù. In shogi engines (notably in adaptations of the open-source engine YaneuraOu), NNUE proved remarkably strong - reportedly reaching play on par with DeepMind‚Äôs AlphaZero in that domain<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. The NNUE concept built on earlier ideas like piece-square tables indexed by king location (an idea used in Kunihito Hoki‚Äôs shogi engine Bonanza), extending them with a neural network that could learn complex piece interactions<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. In essence, NNUE provided a way to combine the efficiency of classical evaluation tables with the flexibility of machine-learned patterns.</p>
<p>After its success in shogi, NNUE was soon applied to chess. A Japanese programmer Hisayori ‚ÄúNodchip‚Äù Noda ported NNUE into a development version of the chess engine Stockfish in early 2020 as a proof of concept<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. This prototype, dubbed Stockfish NNUE, quickly demonstrated dramatically improved playing strength despite some reduction in raw search speed<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. The chess community took note in the summer of 2020, as Stockfish with NNUE began to significantly outperform the conventional Stockfish that used a purely hand-crafted evaluation. On August 6, 2020, the Stockfish team officially merged NNUE into the engine; the resulting release (Stockfish 12) showed a major increase in playing strength over previous versions<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. This marked the first time a top chess engine successfully integrated a neural network evaluation running efficiently on CPU. The elo gain was on the order of 80-100 points in engine testing - a huge leap in a field where progress is often incremental<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúStockfish Chess. ‚ÄúIntroducing NNUE Evaluation.‚Äù https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/‚Äù>[3]</a></sup>.</p>
<p>The adoption of NNUE in Stockfish sparked a broader revolution in computer chess. Many other engine developers quickly experimented with NNUE in their own programs, attracted by the method‚Äôs strength and the relatively small implementation effort required<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. Engines such as Komodo (Dragon), Ethereal, Igel, Scorpio, and various Stockfish derivatives all added NNUE-based evaluations in late 2020 and 2021<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. Even commercial products like Fat Fritz 2 (by ChessBase) leveraged Stockfish‚Äôs NNUE code with customized networks. In a matter of months, efficiently updatable neural nets became the new standard for top engines, largely replacing the decades-long era of handcrafted evaluation functions in classical alpha-beta search engines.</p>
<p>It is worth noting the key contributors in this development. Yu Nasu‚Äôs original NNUE research laid the groundwork<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. The Stockfish integration was enabled by Noda (nodchip) and other collaborators in the open-source community, with support from shogi-engine authors (the Stockfish blog credits ‚Äúnodchip, ynasu87, yaneurao (initial port and NNUE authors)‚Äù among others)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúStockfish Chess. ‚ÄúIntroducing NNUE Evaluation.‚Äù https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/‚Äù>[3]</a></sup>. Early trained networks for chess were produced by community members like gekkehenker and sergiovieri, whose nets were used as defaults in Stockfish 12<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúStockfish Chess. ‚ÄúIntroducing NNUE Evaluation.‚Äù https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/‚Äù>[3]</a></sup>. This collaborative, cross-community effort was a prime example of knowledge transfer from computer shogi to chess. By 2021, Stockfish NNUE decisively won the Top Chess Engine Championship (TCEC), and the hybrid of classical search with NNUE evaluation firmly established itself as the strongest engine paradigm.</p>
<h2 id="technical-design"><a class="markdownIt-Anchor" href="#technical-design"></a> Technical Design</h2>
<p>At its core, an NNUE is a neural network-based evaluation function used inside a chess engine‚Äôs search. Unlike the deep convolutional networks used by systems like AlphaZero or Leela Chess Zero, an NNUE is typically a fully-connected network with a relatively small number of layers. A common architecture (as used in early NNUE for chess) consisted of an input layer feeding into one or two hidden layers, and an output neuron that produces the evaluation score of a position<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. For example, one basic NNUE design has an input vector of length 768 (representing board features), a hidden layer of an arbitrary size (often on the order of a few thousand neurons), and then an output layer that yields a single scalar score<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. The network uses standard nonlinear activation functions (the original NNUE used ReLU or its clipped variants) to introduce non-linearity<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. Notably, the inputs to NNUE are hand-engineered feature indices rather than raw pixels or raw board matrices. In chess, these inputs are derived from piece-square tables - essentially indicators for specific piece types on specific squares, possibly differentiated by the king‚Äôs location or side to move<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. This structured input leverages domain knowledge (like classical evaluation terms) in a way that is amenable to neural processing.</p>
<p>The defining feature of NNUE‚Äôs design is captured in the name ‚Äúefficiently updatable.‚Äù The network is structured to exploit the fact that consecutive chess positions (during a search) differ only by a small number of piece moves. Therefore, instead of recomputing the entire network from scratch for each position, NNUE maintains an accumulator for the first hidden layer that can be incrementally updated when a move is made or unmade<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. In practice, this means when a piece moves, only a few input neurons change (for a typical quiet move, two input neurons change state; for a capture or castling, maybe three or four inputs change)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. The contribution of those changed inputs to the hidden layer is added or subtracted from the accumulator, while unaffected neurons retain their previous values<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. This technique avoids a full recomputation of the hidden layer on every node of the search tree. After updating the first layer incrementally, the subsequent layers (from the hidden layer to output) are small enough to compute from scratch very quickly. This incremental computation is what allows NNUE to be used within a high-speed alpha-beta search without crippling the engine‚Äôs nodes-per-second - a critical requirement that earlier large neural networks could not meet.</p>
<p>To achieve maximum efficiency on standard CPUs, NNUE implementations use low-precision arithmetic and vectorized instructions. The networks are typically quantized to use integers (e.g. 8-bit or 16-bit weights) instead of 32-bit floats<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. Stockfish‚Äôs NNUE, for instance, stores the first-layer weights as 16-bit and later layers as 8-bit values<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. The arithmetic is then optimized with SIMD (Single Instruction, Multiple Data) operations, taking advantage of CPU instruction sets to compute many neuron contributions in parallel<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. These optimizations, combined with the accumulator design, allow NNUE to evaluate millions of positions per second on a CPU, only modestly slower than classical evaluation code. By contrast, a typical deep neural net like Leela Chess Zero‚Äôs requires a GPU for fast inference due to its size and complexity<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. In summary, the NNUE design marries a simple fully-connected neural network with clever engineering (feature-based inputs, incremental updates, and quantized SIMD computation) to create a fast and strong evaluation function suitable for classical search engines.</p>
<h2 id="principles-and-comparison-with-traditional-neural-networks"><a class="markdownIt-Anchor" href="#principles-and-comparison-with-traditional-neural-networks"></a> Principles and Comparison with Traditional Neural Networks</h2>
<p>The fundamental principle behind NNUE is leveraging small state changes for efficient evaluation updates. Traditional neural networks used in chess (such as the deep residual networks in Leela Chess Zero (LC0) or Google DeepMind‚Äôs AlphaZero) treat each position independently - they input the entire board state (often as a multi-plane matrix encoding piece locations or probabilities) into a large network and compute an evaluation (and in LC0‚Äôs case, also move probabilities) from scratch. NNUE differs in several key ways:</p>
<p>NNUE uses a structured input encoding (piece-square indices, often split by side-to-move or king proximity) which is sparse and based on domain-specific features<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. Each input neuron might correspond to something like ‚Äúwhite knight on F3‚Äù or ‚Äúblack queen on D8 with white king on G1‚Äù in some formulations. Only those neurons corresponding to pieces actually present on the board are active. In contrast, LC0‚Äôs network takes a complete board representation (e.g. 8√ó8√óN binary planes or other similar encodings) without such explicit feature indexing. NNUE‚Äôs input design thus embeds chess knowledge into the network‚Äôs structure, whereas LC0‚Äôs deep net learns from more generic representations.</p>
<p>NNUE networks are relatively shallow (commonly 3 or 4 layers in total as noted above) and have on the order of a few million weights<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. LC0‚Äôs neural net by comparison is a deep residual network with dozens of layers and tens of millions of weights, requiring far more computation. The NNUE‚Äôs simplicity is a trade-off made to ensure it can run quickly on CPU. The ‚Äúefficiently updatable‚Äù property only strictly applies to the first layer (since once a non-linear activation is applied, reusing intermediate values further in the network is not straightforward)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. In practice, Stockfish‚Äôs search updates the first hidden layer via the accumulator and then computes the remaining layers fresh<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. Deep networks like LC0 do not have any concept of incremental update; every evaluation is a full forward-pass through the entire network. This makes them far slower per position - LC0 compensates by evaluating fewer positions using intelligent Monte Carlo Tree Search, guided by the network‚Äôs policy outputs.</p>
<p>The way NNUE networks are trained also differs from reinforcement learning approaches used by AlphaZero/Leela. NNUE networks for chess have typically been trained in a supervised manner using annotated position data. In fact, the initial Stockfish NNUE was trained on ‚Äúthe evaluations of millions of positions at moderate search depth‚Äù<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúStockfish Chess. ‚ÄúIntroducing NNUE Evaluation.‚Äù https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/‚Äù>[3]</a></sup>. In other words, millions of chess positions were evaluated with classical Stockfish (or an engine) at some depth, and those evaluation scores became training targets for the neural net. This process is akin to knowledge distillation - the network learns to predict the engine‚Äôs own evaluation. This provided a huge training set without needing self-play from scratch. Subsequent networks have also incorporated reinforcement learning elements or self-play data to further improve: for example, developers experimented with letting engines using NNUE play games against each other or against classical engines to generate new training data, thus blending supervised and reinforcement learning<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>. By contrast, Leela Chess Zero‚Äôs nets are trained via pure reinforcement learning from self-play games (starting from random play and gradually improving), guided by game outcomes and the AlphaZero-style algorithms. In summary, NNUE training tends to use existing chess knowledge or engine analysis as a teacher, whereas LC0 learns by playing millions of games to tune its weights.</p>
<p>Both NNUE-based engines and Leela/AlphaZero ultimately combine neural evaluations with search, but the search paradigms differ. Stockfish with NNUE continues to use alpha-beta pruning and related deterministic search techniques, examining huge game trees with the NNUE providing static scores at leaf nodes (and sometimes mid-search for pruning heuristics)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúStockfish Chess. ‚ÄúIntroducing NNUE Evaluation.‚Äù https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/‚Äù>[3]</a></sup>. Leela‚Äôs approach uses Monte Carlo Tree Search (MCTS) guided by neural net outputs (both value and policy), which means it investigates fewer positions but in a probabilistic, policy-driven way<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessWorld.net. ‚ÄúLeela Chess Zero and Engine Analysis.‚Äù https://chessworld.net/‚Äù>[4]</a></sup>. For a user, one practical difference is that Stockfish NNUE still provides deterministic principal variation lines and numeric scores in centipawns, while Leela‚Äôs evaluations come as win probabilities and its move choice may appear more ‚Äústrategic‚Äù due to the policy guidance. In terms of output, NNUE yields only a single evaluation number (how favorable the position is for a side)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>, whereas Leela‚Äôs network produces both an evaluation and a probability distribution over moves (this distribution is used to bias the search towards promising moves). Despite these differences, at a high level NNUE and Leela‚Äôs net share the role of replacing manual evaluation heuristics with learned knowledge. Modern Stockfish (as of 2023-2025) even incorporated some ideas from deep learning world by using multiple embedded nets or larger nets (up to two hidden layers, etc.) as long as they remain efficient<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessProgramming Wiki. ‚ÄúNNUE.‚Äù https://www.chessprogramming.org/NNUE‚Äù>[2]</a></sup>.</p>
<p>In summary, NNUE can be seen as a hybrid approach: it injects a neural network into a classical engine, preserving the efficient search algorithm. Leela Chess Zero represents the end-to-end neural approach, using a big neural net to evaluate positions (and suggest moves) with a different style of search. Both approaches have yielded top-tier performance, but they differ in resource needs and style. Notably, NNUE‚Äôs big advantage is that it runs on CPU and slots into existing chess engines, whereas LC0 typically needs GPU acceleration to reach its full potential<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>. This difference in hardware reliance is a direct outcome of the design philosophy behind NNUE - to be lightweight enough for CPU inference.</p>
<h3 id="nnue-vs-leela-chess-zero"><a class="markdownIt-Anchor" href="#nnue-vs-leela-chess-zero"></a> NNUE vs. Leela Chess Zero</h3>
<p>NNUEs can leverage extremely fast alpha-beta search; Stockfish NNUE examines far more nodes per second than Leela can, which means it won‚Äôt miss brute-force wins (e.g. long tactical sequences) as easily. It also runs on commodity CPUs without needing a high-end GPU - very practical for most users or for running on servers. Additionally, the hybrid approach benefited from decades of search optimizations (pruning, move ordering, endgame tablebases, etc.), which were all retained. In engine-vs-engine play, Stockfish NNUE has maintained an edge in strength over LC0 in most settings, especially when each is on its ‚Äúideal‚Äù hardware (Stockfish on CPU cluster vs Leela on GPU cluster)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessWorld.net. ‚ÄúLeela Chess Zero and Engine Analysis.‚Äù https://chessworld.net/‚Äù>[4]</a></sup>. This suggests the combination of deep search and a decent neural eval is a potent mix.</p>
<p>Leela‚Äôs approach has an intrinsic elegance - the neural network learns everything (evaluation and move preferences) from scratch, potentially noticing long-term strategic plans more naturally. LC0‚Äôs evaluations are on a scale of win probabilities, which sometimes align better with practical game outcomes (whereas NNUE‚Äôs centipawn scores are not directly probabilities). In some types of positions, especially those requiring nuanced judgment without deep calculation (say fortress scenarios or long-term sacrifices), Leela‚Äôs policy-driven search can sometimes find moves that the brute-force approach might overlook until very deep. Another aspect is that Leela‚Äôs network, being larger, might encode patterns (like complex positional themes) that a smaller NNUE might not capture unless explicitly present in training data. From a development perspective, the LC0 style also opens research into new network architectures (e.g. transformers, as some works suggest<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúWikipedia contributors. ‚ÄúEfficiently updatable neural network.‚Äù Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network‚Äù>[1]</a></sup>), whereas NNUE is relatively constrained by needing to remain small and updatable. Finally, LC0‚Äôs use of GPU hardware means it can scale with improved hardware (bigger and faster GPUs can directly accelerate it), whereas scaling Stockfish beyond a point yields diminishing returns due to memory and parallel search limits. In summary, NNUE‚Äôs approach is more pragmatic and engineered for immediate strength, while Leela‚Äôs is more holistic, potentially offering insights into chess that a brute-force engine might not find as intuitively.</p>
<h2 id="grandmasters-perspectives-on-neural-network-engines"><a class="markdownIt-Anchor" href="#grandmasters-perspectives-on-neural-network-engines"></a> Grandmasters‚Äô Perspectives on Neural-Network Engines</h2>
<p>The rise of NNUE and neural-network engines has not only impacted computer chess competitions but also influenced top human players and their preparation. Elite grandmasters have closely followed these developments and often commented on them:</p>
<p>Magnus Carlsen has expressed great interest in neural net engines. In fact, he was quick to incorporate them into his analytical toolkit. In a 2021 Norwegian interview, Carlsen noted that he started using Leela ‚Äúvery quickly, before all of his competitors did,‚Äù and he partially credited this early adoption for one of the best performance periods of his career<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúReddit. ‚ÄúMagnus Carlsen interview: Chess intuition, alcohol.‚Äù https://www.reddit.com/r/chess/comments/p7d4he/magnus_carlsen_interview_chess_intuition_alcohol/‚Äù>[5]</a></sup>. This indicates that Carlsen gained practical insights or novelties from Leela‚Äôs style of analysis that others hadn‚Äôt yet accessed. He has also been inspired by AlphaZero‚Äôs games - Carlsen remarked that AlphaZero‚Äôs willingness to sacrifice material for long-term advantages was fascinating, and it offered a ‚Äúdifferent perspective‚Äù on the game that influenced his own play<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúGlasp. ‚ÄúCarlsen on AlphaZero.‚Äù https://glasp.co/youtube/z_wXRCMY1nE‚Äù>[6]</a></sup>. In interviews, Carlsen described being ‚Äúhugely inspired‚Äù by the creative, sometimes ‚Äúalien‚Äù strategies shown by neural network engines, suggesting that studying these engines helped him evolve his style. It‚Äôs telling that the World Champion‚Äôs team in recent years uses a mix of traditional engines and neural nets for preparation. For example, during the 2018 World Championship match, both Carlsen‚Äôs and challenger Fabiano Caruana‚Äôs teams made use of Leela Chess Zero for opening preparation, even though Leela at the time was still relatively weak in some technical positions<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChess.com. ‚ÄúFabiano Caruana On Playing A World Championship, On Carlsen-Nepomniachtchi, And More.‚Äù https://www.chess.com/news/view/fabiano-caruana-interview-carlsen-nepomniachtchi‚Äù>[7]</a></sup>. Caruana noted that in 2018 ‚Äúnobody was really using Leela then‚Äù but they decided to try it, and while it had endgame weaknesses and occasional tactical blind spots, ‚Äúit was also helpful in preparation‚Äù<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChess.com. ‚ÄúFabiano Caruana On Playing A World Championship, On Carlsen-Nepomniachtchi, And More.‚Äù https://www.chess.com/news/view/fabiano-caruana-interview-carlsen-nepomniachtchi‚Äù>[7]</a></sup>. This early use by top players underscores the value of Leela‚Äôs unique ‚Äúhuman-like‚Äù ideas, which could complement the brute-force analysis of engines like Stockfish.</p>
<p>Hikaru Nakamura has also engaged with neural-net engines, both in analysis and even in exhibition matches. He played a public match against Leela Chess Zero (receiving knight odds) in 2020, which showcased the engine‚Äôs strength even with a material handicap. Nakamura, known for his pragmatic and sometimes skeptical view on hype, essentially recognized that neural net engines brought something new to the table. In a later interview, he mentioned that engines like Leela and AlphaZero introduced ideas that ‚Äúcompletely changed the way top players analyze certain positions,‚Äù especially in terms of dynamic sacrifices and pawn structure strategies. Like many of his peers, Nakamura uses engines extensively in his game preparation and has adapted to the ‚Äúnew wisdom‚Äù they provide - for instance, being more willing to consider long-term piece sacrifices that classical engines used to undervalue.</p>
<p>Garry Kasparov, who famously was the first world champion defeated by a computer (Deep Blue in 1997), has spoken very positively about the new generation of AI engines. He called AlphaZero‚Äôs style ‚Äúa pleasure to watch,‚Äù noting that unlike the old brute-force machines, AlphaZero played ‚Äúvery aggressive, creative‚Äù chess<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label="The Guardian. ‚Äú‚ÄòCreative‚Äô AlphaZero leads way for chess computers and, maybe, science.‚Äù https://www.theguardian.com/sport/2018/dec/11/creative-alphazero-leads-way-chess-computers-science<br>
">[8]</a></sup>. Kasparov was struck by how the machine‚Äôs play ‚Äúsounds rather human‚Äù in its qualities - a testament to how neural nets brought a form of intuition to computer chess<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label="The Guardian. ‚Äú‚ÄòCreative‚Äô AlphaZero leads way for chess computers and, maybe, science.‚Äù https://www.theguardian.com/sport/2018/dec/11/creative-alphazero-leads-way-chess-computers-science<br>
">[8]</a></sup>. He labeled AlphaZero‚Äôs emergence a ‚Äúreal breakthrough‚Äù and has since advocated that humans should learn from these AI. Kasparov‚Äôs endorsement is significant: after initially feeling bitter about computers, he now embraces the neural net engines as tools that push chess understanding forward and even as analogies for broader AI potential.</p>
<p>Players like Vishwanathan Anand and Fabiano Caruana have also discussed how the engine landscape changed around 2018-2020. Caruana mentioned that the ‚Äúrate of improvement [in engines] is amazing‚Äù and that by 2021 the level of preparation with engines had become astronomically high<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChess.com. ‚ÄúFabiano Caruana On Playing A World Championship, On Carlsen-Nepomniachtchi, And More.‚Äù https://www.chess.com/news/view/fabiano-caruana-interview-carlsen-nepomniachtchi‚Äù>[7]</a></sup>. He noted that mistakes in analysis that might have occurred a few years ago ‚Äúwould never happen, three years later‚Äù because the neural-net-enhanced engines ‚Äúspot it instantly‚Äù<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChess.com. ‚ÄúFabiano Caruana On Playing A World Championship, On Carlsen-Nepomniachtchi, And More.‚Äù https://www.chess.com/news/view/fabiano-caruana-interview-carlsen-nepomniachtchi‚Äù>[7]</a></sup>. This underscores that modern engines (Stockfish NNUE, Leela, etc.) have improved so much that top humans rely on them to double-check every line. Ding Liren and Ian Nepomniachtchi, who contested the 2023 World Championship, both prepared with a combination of Stockfish (with NNUE) and Leela, often cross-verifying critical positions with both types of engines to gather diverse insights.</p>
<p>In practical terms, neural network engines have influenced top GMs in areas like opening preparation and positional understanding. Neural network chess engines introduced a wealth of unorthodox opening ideas, some lines once considered dubious were found to be playable thanks to deep neural eval, and vice versa. For instance, neural nets often favor dynamic piece activity over material, leading to a re-examination of gambit lines or long-term sacrifices. Grandmasters have incorporated many of these engine-approved ideas into their repertoires. Additionally, studying the games played between Stockfish and Leela (in events like TCEC) has become a way for GMs to enrich their intuition. As one article noted, ‚Äúevery serious chess player - amateur or professional - studies the games of these engines to expand their understanding.‚Äù<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class=‚Äúhint‚Äìtop hint‚Äìerror hint‚Äìmedium hint‚Äìrounded hint‚Äìbounce‚Äù aria-label=‚ÄúChessWorld.net. ‚ÄúLeela Chess Zero and Engine Analysis.‚Äù https://chessworld.net/‚Äù>[4]</a></sup> This sentiment is widely shared: the engine ‚Äúsparring partners‚Äù are simply too strong and insightful to ignore.</p>
<h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2>
<p>The Efficiently Updatable Neural Network (NNUE) represents a revolutionary advancement in computer chess that successfully bridged the gap between classical search algorithms and modern neural network technology. By combining the efficiency of traditional alpha-beta search with the pattern recognition capabilities of neural networks, NNUE created a new paradigm that has fundamentally transformed the landscape of chess engines.</p>
<p>The success of NNUE demonstrates the power of hybrid approaches in artificial intelligence. Rather than replacing classical methods entirely, NNUE enhanced them by providing more sophisticated evaluation functions that could learn from vast amounts of data while maintaining the computational efficiency required for deep search. This approach proved so effective that it quickly became the standard for top chess engines, marking the end of the era of purely handcrafted evaluation functions.</p>
<p>The impact of NNUE extends beyond mere technical achievement. It has influenced how top human players approach chess preparation and analysis, creating a symbiotic relationship between human creativity and machine intelligence. Grandmasters like Magnus Carlsen have embraced these tools, finding inspiration in the novel strategies and insights that neural networks can provide.</p>
<p>Looking forward, NNUE represents just the beginning of what‚Äôs possible when combining classical algorithms with modern machine learning techniques. The principles developed for NNUE-efficient incremental updates, quantized networks, and hybrid architectures-have applications far beyond chess, potentially influencing other domains where real-time decision making requires both speed and sophistication.</p>
<p>The story of NNUE is ultimately one of collaboration and innovation, showing how ideas from different communities (Japanese shogi programmers, open-source chess developers, and machine learning researchers) can come together to create something greater than the sum of its parts. As we continue to explore the intersection of classical algorithms and neural networks, NNUE stands as a testament to the power of thoughtful engineering and the potential for AI to enhance rather than replace human capabilities.</p>
<h2 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h2>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wikipedia contributors. "Efficiently updatable neural network." Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network<a href="#fnref:1" rev="footnote"> ‚Ü©</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">ChessProgramming Wiki. "NNUE." https://www.chessprogramming.org/NNUE<a href="#fnref:2" rev="footnote"> ‚Ü©</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Stockfish Chess. "Introducing NNUE Evaluation." https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/<a href="#fnref:3" rev="footnote"> ‚Ü©</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">ChessWorld.net. "Leela Chess Zero and Engine Analysis." https://chessworld.net/<a href="#fnref:4" rev="footnote"> ‚Ü©</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Reddit. "Magnus Carlsen interview: Chess intuition, alcohol." https://www.reddit.com/r/chess/comments/p7d4he/magnus_carlsen_interview_chess_intuition_alcohol/<a href="#fnref:5" rev="footnote"> ‚Ü©</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Glasp. "Carlsen on AlphaZero." https://glasp.co/youtube/z_wXRCMY1nE<a href="#fnref:6" rev="footnote"> ‚Ü©</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chess.com. "Fabiano Caruana On Playing A World Championship, On Carlsen-Nepomniachtchi, And More." https://www.chess.com/news/view/fabiano-caruana-interview-carlsen-nepomniachtchi<a href="#fnref:7" rev="footnote"> ‚Ü©</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">The Guardian. "'Creative' AlphaZero leads way for chess computers and, maybe, science." https://www.theguardian.com/sport/2018/dec/11/creative-alphazero-leads-way-chess-computers-science<a href="#fnref:8" rev="footnote"> ‚Ü©</a></span></li></ol></div></div>
    </div>
</article>

    </main>
        <footer>
    <p>This blog was made with <a href="https://hexo.io/">Hexo</a> and <a href="https://simplecss.org">Simple.css</a>.<br>
    Content is available under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 </a> unless noted otherwise.</p>
    <p>

        <a href="/atom.xml">
        <svg class="icon" aria-hidden="true" width="16" height="16" viewbox="0 -1 16 16" xmlns="http://www.w3.org/2000/svg">
        <path fill="currentColor" d="M4.259 23.467c-2.35 0-4.259 1.917-4.259 4.252 0 2.349 1.909 4.244 4.259 4.244 2.358 0 4.265-1.895 4.265-4.244-0-2.336-1.907-4.252-4.265-4.252zM0.005 10.873v6.133c3.993 0 7.749 1.562 10.577 4.391 2.825 2.822 4.384 6.595 4.384 10.603h6.16c-0-11.651-9.478-21.127-21.121-21.127zM0.012 0v6.136c14.243 0 25.836 11.604 25.836 25.864h6.152c0-17.64-14.352-32-31.988-32z" transform="scale(0.46)"/></svg>RSS Feed</a>

    ‚Ä¢ <a href="https://github.com/madnight/blog"><svg class="icon" aria-hidden="true" width="16" height="16" viewbox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z" fill="currentColor"/>
</svg>Source</a>
    </p>
    </footer>

    </body>
</html>
